{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "19TD3ZO4cGrp7VwOXfnoWfmlRxvqiHT59",
      "authorship_tag": "ABX9TyNMnu+wbnMX00h94vj6A9be",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priya9112/Null-Class-Data-Science-Internship-/blob/main/TASK_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TASK - 5**\n",
        " *Create a feature to translate the language with a combination of two languages at the same time . We should be able to convert the 2 different languages at the same time . translate English to French and Hindi at the same time . This model should work only for 10 letter English words . If we enter below 10 letters or above 10 letters it should not work.*"
      ],
      "metadata": {
        "id": "6dAp3ZB2MiPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Automated Dataset Creation**"
      ],
      "metadata": {
        "id": "ll4Xs63eMxJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==3.1.0a0\n",
        "import nltk\n",
        "from googletrans import Translator\n",
        "\n",
        "nltk.download('words')\n",
        "from nltk.corpus import words\n",
        "\n",
        "ten_letter_words = [word for word in words.words() if len(word) == 10]\n",
        "translator = Translator()\n",
        "\n",
        "english_words = []\n",
        "french_translations = []\n",
        "hindi_translations = []\n",
        "\n",
        "max_words = 300\n",
        "\n",
        "for i, word in enumerate(ten_letter_words):\n",
        "    if i >= max_words:\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        # French and Hindi\n",
        "        french_translation = translator.translate(word, src='en', dest='fr').text\n",
        "        hindi_translation = translator.translate(word, src='en', dest='hi').text\n",
        "\n",
        "        english_words.append(word)\n",
        "        french_translations.append(french_translation)\n",
        "        hindi_translations.append(hindi_translation)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error translating word '{word}': {e}\")\n",
        "        continue\n",
        "\n",
        "with open('english_words.txt', 'w', encoding='utf-8') as eng_file, \\\n",
        "     open('french_translations.txt', 'w', encoding='utf-8') as fr_file, \\\n",
        "     open('hindi_translations.txt', 'w', encoding='utf-8') as hi_file:\n",
        "\n",
        "    for eng, fr, hi in zip(english_words, french_translations, hindi_translations):\n",
        "        eng_file.write(eng + '\\n')\n",
        "        fr_file.write(fr + '\\n')\n",
        "        hi_file.write(hi + '\\n')\n",
        "\n",
        "print(\"English:\", english_words[:10])\n",
        "print(\"French:\", french_translations[:10])\n",
        "print(\"Hindi:\", hindi_translations[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzn16-O1QmkS",
        "outputId": "27143435-1142-489c-d87f-11f8e6e4c772"
      },
      "execution_count": 5,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==3.1.0a0)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2024.7.4)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading hstspreload-2024.8.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2024.8.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16353 sha256=65cbd35bfdae8aa143629785b536efb53412d110e9fb17bff54f3b31cf2922e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/5d/3c/8477d0af4ca2b8b1308812c09f1930863caeebc762fe265a95\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.7\n",
            "    Uninstalling idna-3.7:\n",
            "      Successfully uninstalled idna-3.7\n",
            "Successfully installed chardet-3.0.4 googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.8.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: ['abalienate', 'abaptiston', 'abasedness', 'abbeystede', 'abbreviate', 'abdication', 'abdicative', 'abdominous', 'aberdevine', 'Aberdonian']\n",
            "French: ['abaliéner', 'abaptiston', 'avilissement', 'abbaye', 'abréger', 'abdication', 'abdicatif', 'ventripotent', 'aberdevine', 'Aberdonien']\n",
            "Hindi: ['निरपेक्ष करना', 'abaptiston', 'अपमान', 'abbeystede', 'संक्षिप्त', 'त्याग', 'त्यागनेवाला', 'उदर संबंधी', 'aberdevine', 'एबर्डोनियन']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code to Translate**"
      ],
      "metadata": {
        "id": "eRuTbm7BM4dI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "def load_dataset(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read().splitlines()\n",
        "\n",
        "english_words = load_dataset('/content/english_words.txt')\n",
        "french_translations = load_dataset('/content/french_translations.txt')\n",
        "hindi_translations = load_dataset('/content/hindi_translations.txt')\n",
        "\n",
        "translation_dict = {eng: {'fr': fr, 'hi': hi} for eng, fr, hi in zip(english_words, french_translations, hindi_translations)}\n",
        "\n",
        "def translate_to_french_and_hindi(word):\n",
        "    if len(word) != 10:\n",
        "        return \"Error: The word must be exactly 10 letters long.\"\n",
        "    if word in translation_dict:\n",
        "        return translation_dict[word]\n",
        "    else:\n",
        "        return \"Error: Word not found in the dataset.\"\n",
        "\n",
        "word_to_translate1 = input(\"Enter the first word to translate: \")\n",
        "translations1 = translate_to_french_and_hindi(word_to_translate1)\n",
        "\n",
        "if isinstance(translations1, dict):\n",
        "    print(f\"English: {word_to_translate1}\")\n",
        "    print(f\"French: {translations1['fr']}\")\n",
        "    print(f\"Hindi: {translations1['hi']}\")\n",
        "else:\n",
        "    print(f\"{word_to_translate1}: {translations1}\")\n",
        "\n",
        "word_to_translate2 = input(\"\\nEnter the second word to translate: \")\n",
        "translations2 = translate_to_french_and_hindi(word_to_translate2)\n",
        "\n",
        "if isinstance(translations2, dict):\n",
        "    print(f\"\\nEnglish: {word_to_translate2}\")\n",
        "    print(f\"French: {translations2['fr']}\")\n",
        "    print(f\"Hindi: {translations2['hi']}\")\n",
        "else:\n",
        "    print(f\"{word_to_translate2}: {translations2}\")\n"
      ],
      "metadata": {
        "id": "sz6T8HiMROTT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e808262-3691-46a5-d281-3d5acf8890b3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the first word to translate: abbreviate\n",
            "English: abbreviate\n",
            "French: abréger\n",
            "Hindi: संक्षिप्त\n",
            "\n",
            "Enter the second word to translate: Additionally \n",
            "Additionally : Error: The word must be exactly 10 letters long.\n"
          ]
        }
      ]
    }
  ]
}