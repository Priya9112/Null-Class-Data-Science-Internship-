{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1GwlAazKhjilCe7RAH3ao5veUOi7SpzJG",
      "authorship_tag": "ABX9TyNlUHEg3mL9Ptc9l7yRf7Ng",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priya9112/Null-Class-Data-Science-Internship-/blob/main/French_Tamil_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NN5xsqKaj2lY",
        "outputId": "3c21e63e-5b9a-44eb-a824-daffca2a989f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Embedding, GRU, LSTM, Bidirectional, Dropout, Activation, TimeDistributed, RepeatVector\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy"
      ],
      "metadata": {
        "id": "xkoREc_zu-Id"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7HMOVoDLfEiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "metadata": {
        "id": "SZ1r-KiC4PjW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd46ed7d-e2ef-40f5-ff4b-5e2487c09815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 2699494265312168631\n",
            "xla_global_id: -1\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "    input_file = path\n",
        "    with open(input_file, \"r\", encoding='utf-8') as f:\n",
        "        data = f.read()\n",
        "    return data.split('\\n')\n",
        "\n",
        "french_sentences = load_data('/content/drive/MyDrive/french-tamil/fr.txt')\n",
        "tamil_sentences = load_data('/content/drive/MyDrive/french-tamil/ta.txt')"
      ],
      "metadata": {
        "id": "klxNsQcc4khc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "french_sentences[:5]"
      ],
      "metadata": {
        "id": "rKbfLTYu43dR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb9abe4f-a977-4009-eecf-e2cabb078066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['À la moitié du 16e siècle, les Italiens étaient fascinés par un type de chanteur masculin dont le registre incroyable contenait des notes auparavant considérées comme impossibles pour les hommes.',\n",
              " 'Néanmoins, ce don coûtait cher.',\n",
              " 'Pour empêcher que leur voix se casse, ces chanteurs étaient castrés avant leur puberté, freinant les processus hormonaux qui rendraient leur voix grave.',\n",
              " \"Appelés castrats, leur voix douce et angélique était célèbre en Europe, jusqu'à ce que l'opération cruelle soit proscrite vers l'an 1800.\",\n",
              " \"Bien qu'une voix d'enfant accède à un registre musical extraordinaire, une voix développée naturellement est à même d'avoir une variété incroyable.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
        "tamil_words_counter = collections.Counter([word for sentence in tamil_sentences for word in sentence.split()])\n",
        "\n",
        "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
        "print('{} unique French words.'.format(len(french_words_counter)))\n",
        "print('10 Most common words in the French dataset:')\n",
        "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')\n",
        "\n",
        "print()\n",
        "print('{} Tamil words.'.format(len([word for sentence in tamil_sentences for word in sentence.split()])))\n",
        "print('{} unique Tamil words.'.format(len(tamil_words_counter)))\n",
        "print('10 Most common words in the Tamil dataset:')\n",
        "print('\"' + '\" \"'.join(list(zip(*tamil_words_counter.most_common(10)))[0]) + '\"')"
      ],
      "metadata": {
        "id": "zAOXNux05bH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d659100e-ec03-4304-a4d4-34419a5ec2e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7152230 French words.\n",
            "225805 unique French words.\n",
            "10 Most common words in the French dataset:\n",
            "\"de\" \"la\" \"et\" \"les\" \"que\" \"à\" \"le\" \"des\" \"un\" \"en\"\n",
            "\n",
            "129862 Tamil words.\n",
            "41275 unique Tamil words.\n",
            "10 Most common words in the Tamil dataset:\n",
            "\"ஒரு\" \"நான்\" \"இந்த\" \"நாம்\" \"இது\" \"அது\" \"நீங்கள்\" \"ஆனால்\" \"என்று\" \"மற்றும்\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "\n",
        "# French sentences\n",
        "french_sentences = [\n",
        "    'Le renard brun saute par-dessus le chien paresseux.',\n",
        "    'Par Jove, mon étude rapide de lexicographie a gagné un prix.',\n",
        "    'Ceci est une phrase courte.']\n",
        "\n",
        "# Tamil sentences\n",
        "tamil_sentences = [\n",
        "    'அந்த கருமை நரி கண்டது ஓயாத நாயை தாண்டியது.',\n",
        "    'ஜோவேயின் கீழ், என் விரைவான இலக்கியக் கல்வி பரிசை வென்றது.',\n",
        "    'இது ஒரு குறுகிய வாக்கியம்.']\n",
        "\n",
        "def tokenize(x):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(x)\n",
        "    return tokenizer.texts_to_sequences(x), tokenizer\n",
        "\n",
        "french_tokenized, french_tokenizer = tokenize(french_sentences)\n",
        "tamil_tokenized, tamil_tokenizer = tokenize(tamil_sentences)\n",
        "\n",
        "print(french_tokenizer.word_index)\n",
        "print()\n",
        "for sample_i, (sent, token_sent) in enumerate(zip(french_sentences, french_tokenized)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(sent))\n",
        "    print('  Output: {}'.format(token_sent))\n",
        "\n",
        "print()\n",
        "print(tamil_tokenizer.word_index)\n",
        "print()\n",
        "for sample_i, (sent, token_sent) in enumerate(zip(tamil_sentences, tamil_tokenized)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(sent))\n",
        "    print('  Output: {}'.format(token_sent))"
      ],
      "metadata": {
        "id": "9KJ2b8me5kfm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3836503-ddda-4008-80c1-9012af9e51bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'le': 1, 'par': 2, 'renard': 3, 'brun': 4, 'saute': 5, 'dessus': 6, 'chien': 7, 'paresseux': 8, 'jove': 9, 'mon': 10, 'étude': 11, 'rapide': 12, 'de': 13, 'lexicographie': 14, 'a': 15, 'gagné': 16, 'un': 17, 'prix': 18, 'ceci': 19, 'est': 20, 'une': 21, 'phrase': 22, 'courte': 23}\n",
            "\n",
            "Sequence 1 in x\n",
            "  Input:  Le renard brun saute par-dessus le chien paresseux.\n",
            "  Output: [1, 3, 4, 5, 2, 6, 1, 7, 8]\n",
            "Sequence 2 in x\n",
            "  Input:  Par Jove, mon étude rapide de lexicographie a gagné un prix.\n",
            "  Output: [2, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
            "Sequence 3 in x\n",
            "  Input:  Ceci est une phrase courte.\n",
            "  Output: [19, 20, 21, 22, 23]\n",
            "\n",
            "{'அந்த': 1, 'கருமை': 2, 'நரி': 3, 'கண்டது': 4, 'ஓயாத': 5, 'நாயை': 6, 'தாண்டியது': 7, 'ஜோவேயின்': 8, 'கீழ்': 9, 'என்': 10, 'விரைவான': 11, 'இலக்கியக்': 12, 'கல்வி': 13, 'பரிசை': 14, 'வென்றது': 15, 'இது': 16, 'ஒரு': 17, 'குறுகிய': 18, 'வாக்கியம்': 19}\n",
            "\n",
            "Sequence 1 in x\n",
            "  Input:  அந்த கருமை நரி கண்டது ஓயாத நாயை தாண்டியது.\n",
            "  Output: [1, 2, 3, 4, 5, 6, 7]\n",
            "Sequence 2 in x\n",
            "  Input:  ஜோவேயின் கீழ், என் விரைவான இலக்கியக் கல்வி பரிசை வென்றது.\n",
            "  Output: [8, 9, 10, 11, 12, 13, 14, 15]\n",
            "Sequence 3 in x\n",
            "  Input:  இது ஒரு குறுகிய வாக்கியம்.\n",
            "  Output: [16, 17, 18, 19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "def pad(x, length=None):\n",
        "    if length is None:\n",
        "        length = max([len(sentence) for sentence in x])\n",
        "    return pad_sequences(x, maxlen=length, padding='post')\n",
        "\n",
        "french_padded = pad(french_tokenized)\n",
        "tamil_padded = pad(tamil_tokenized)\n",
        "\n",
        "for sample_i, (token_sent, pad_sent) in enumerate(zip(french_tokenized, french_padded)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(np.array(token_sent)))\n",
        "    print('  Output: {}'.format(pad_sent))\n",
        "\n",
        "for sample_i, (token_sent, pad_sent) in enumerate(zip(tamil_tokenized, tamil_padded)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(np.array(token_sent)))\n",
        "    print('  Output: {}'.format(pad_sent))"
      ],
      "metadata": {
        "id": "MHyqVXGZDP5N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1fd6a87-2825-417c-b801-0a3496a63377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence 1 in x\n",
            "  Input:  [1 3 4 5 2 6 1 7 8]\n",
            "  Output: [1 3 4 5 2 6 1 7 8 0 0]\n",
            "Sequence 2 in x\n",
            "  Input:  [ 2  9 10 11 12 13 14 15 16 17 18]\n",
            "  Output: [ 2  9 10 11 12 13 14 15 16 17 18]\n",
            "Sequence 3 in x\n",
            "  Input:  [19 20 21 22 23]\n",
            "  Output: [19 20 21 22 23  0  0  0  0  0  0]\n",
            "Sequence 1 in x\n",
            "  Input:  [1 2 3 4 5 6 7]\n",
            "  Output: [1 2 3 4 5 6 7 0]\n",
            "Sequence 2 in x\n",
            "  Input:  [ 8  9 10 11 12 13 14 15]\n",
            "  Output: [ 8  9 10 11 12 13 14 15]\n",
            "Sequence 3 in x\n",
            "  Input:  [16 17 18 19]\n",
            "  Output: [16 17 18 19  0  0  0  0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(x,y):\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "\n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
        "\n",
        "preproc_french_sentences, preproc_tamil_sentences, french_tokenizer, tamil_tokenizer = preprocess(french_sentences, tamil_sentences)\n",
        "\n",
        "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
        "max_tamil_sequence_length = preproc_tamil_sentences.shape[1]\n",
        "french_vocab_size = len(french_tokenizer.word_index)\n",
        "tamil_vocab_size = len(tamil_tokenizer.word_index)\n",
        "\n",
        "print('Data Preprocessed')\n",
        "print(\"Max French sentence length:\", max_french_sequence_length)\n",
        "print(\"Max Tamil sentence length:\", max_tamil_sequence_length)\n",
        "print(\"French vocabulary size:\", french_vocab_size)\n",
        "print(\"Tamil vocabulary size:\", tamil_vocab_size)"
      ],
      "metadata": {
        "id": "u2TVTpfd7QWq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76267dda-bef9-4404-f421-bdb723bffcbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Preprocessed\n",
            "Max French sentence length: 11\n",
            "Max Tamil sentence length: 8\n",
            "French vocabulary size: 23\n",
            "Tamil vocabulary size: 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def logits_to_text(logits, tokenizer):\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
      ],
      "metadata": {
        "id": "ffQpsMJGBosu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, TimeDistributed, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "import numpy as np\n",
        "\n",
        "def simple_model(input_shape, output_sequence_length, french_vocab_size, tamil_vocab_size):\n",
        "    learning_rate = 0.005\n",
        "    model = Sequential()\n",
        "    model.add(GRU(256, input_shape=input_shape[1:], return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(tamil_vocab_size, activation='softmax')))\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "preproc_tamil_sentences = np.maximum(preproc_tamil_sentences - 1, 0)\n",
        "tmp_x = pad(preproc_french_sentences, max_tamil_sequence_length)\n",
        "tmp_x = tmp_x.reshape((-1, preproc_tamil_sentences.shape[-2], 1))\n",
        "\n",
        "simple_rnn_model = simple_model(\n",
        "    tmp_x.shape,\n",
        "    max_tamil_sequence_length,\n",
        "    french_vocab_size,\n",
        "    tamil_vocab_size\n",
        ")\n",
        "\n",
        "simple_rnn_model.fit(tmp_x, preproc_tamil_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "rE7qzluR99fW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1745895f-d086-4753-ae8b-5e87d821ffce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 3s 3s/step - loss: 2.9242 - accuracy: 0.0625 - val_loss: 3.7751 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 2.7888 - accuracy: 0.1875 - val_loss: 4.1709 - val_accuracy: 0.5000\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 2.4623 - accuracy: 0.1250 - val_loss: 4.5870 - val_accuracy: 0.5000\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 2.6174 - accuracy: 0.1875 - val_loss: 4.9113 - val_accuracy: 0.5000\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 2.5661 - accuracy: 0.0625 - val_loss: 5.2899 - val_accuracy: 0.5000\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 2.2485 - accuracy: 0.3125 - val_loss: 5.6613 - val_accuracy: 0.5000\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 2.4285 - accuracy: 0.1875 - val_loss: 6.0531 - val_accuracy: 0.5000\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.9730 - accuracy: 0.2500 - val_loss: 6.3529 - val_accuracy: 0.5000\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 2.2442 - accuracy: 0.3125 - val_loss: 6.6234 - val_accuracy: 0.5000\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 2.1126 - accuracy: 0.3750 - val_loss: 6.9222 - val_accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f44a4d41e10>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Prediciton:\")\n",
        "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], tamil_tokenizer))\n",
        "\n",
        "print(\"\\nCorrect Translation:\")\n",
        "print(tamil_sentences[:1])\n",
        "\n",
        "print('\\nOriginal text:')\n",
        "print(french_sentences[:1])"
      ],
      "metadata": {
        "id": "FASGwW4lFE0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd4660d1-baca-4b73-d41e-0a35d69f8341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediciton:\n",
            "1/1 [==============================] - 0s 410ms/step\n",
            "தாண்டியது நரி ஓயாத நாயை <PAD> ஓயாத <PAD> <PAD>\n",
            "\n",
            "Correct Translation:\n",
            "['அந்த கருமை நரி கண்டது ஓயாத நாயை தாண்டியது.']\n",
            "\n",
            "Original text:\n",
            "['Le renard brun saute par-dessus le chien paresseux.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional, GRU, TimeDistributed, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "def bd_model(input_shape, tamil_vocab_size):\n",
        "    learning_rate = 0.005\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(GRU(128, return_sequences=True), input_shape=input_shape[1:]))\n",
        "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(tamil_vocab_size + 1, activation='softmax')))\n",
        "\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "french_tokenized = [[1, 2, 3], [4, 5, 6, 7, 8], [9, 10, 11, 12, 13, 14]]\n",
        "tamil_padded = [[1, 2, 3, 4, 5, 6, 7, 8], [4, 5, 6, 7, 8, 9, 10, 11], [9, 10, 11, 12, 13, 14, 15, 16]]\n",
        "\n",
        "max_tamil_sequence_length = max(len(seq) for seq in tamil_padded)\n",
        "tmp_x = pad_sequences(french_tokenized, maxlen=max_tamil_sequence_length, padding='post')\n",
        "tamil_padded = pad_sequences(tamil_padded, maxlen=max_tamil_sequence_length, padding='post')\n",
        "tamil_padded = np.array(tamil_padded) - 1\n",
        "print(\"Shape of tmp_x before reshape:\", tmp_x.shape)\n",
        "print(\"Shape of tamil_padded:\", tamil_padded.shape)\n",
        "tmp_x = tmp_x.reshape((tamil_padded.shape[0], tmp_x.shape[1], 1))\n",
        "\n",
        "tamil_vocab_size = max(np.max(tamil_padded), 0) + 1\n",
        "\n",
        "bd_rnn_model = bd_model(\n",
        "    tmp_x.shape,\n",
        "    tamil_vocab_size)\n",
        "\n",
        "print(bd_rnn_model.summary())\n",
        "\n",
        "bd_rnn_model.fit(tmp_x, tamil_padded, batch_size=1024, epochs=10, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "rq0dG-v7DThq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe609ab-cc8c-4a3d-ad53-51e45c81620c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of tmp_x before reshape: (3, 8)\n",
            "Shape of tamil_padded: (3, 8)\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional (Bidirection  (None, 8, 256)            100608    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDi  (None, 8, 1024)           263168    \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 1024)           0         \n",
            "                                                                 \n",
            " time_distributed_3 (TimeDi  (None, 8, 17)             17425     \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 381201 (1.45 MB)\n",
            "Trainable params: 381201 (1.45 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - 5s 5s/step - loss: 2.8598 - accuracy: 0.0000e+00 - val_loss: 4.5685 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 2.4815 - accuracy: 0.1875 - val_loss: 7.2611 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 2.4208 - accuracy: 0.1875 - val_loss: 9.6469 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 2.2565 - accuracy: 0.1875 - val_loss: 11.1544 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 2.1737 - accuracy: 0.2500 - val_loss: 12.0813 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 2.2010 - accuracy: 0.1875 - val_loss: 12.7461 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 2.0379 - accuracy: 0.2500 - val_loss: 13.1124 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.7452 - accuracy: 0.2500 - val_loss: 13.9046 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.7260 - accuracy: 0.3750 - val_loss: 14.6434 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 1.7077 - accuracy: 0.5000 - val_loss: 15.2261 - val_accuracy: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f44a7391f00>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Prediciton:\")\n",
        "print(logits_to_text(bd_rnn_model.predict(tmp_x[:1])[0], tamil_tokenizer))\n",
        "\n",
        "print(\"\\nCorrect Translation:\")\n",
        "print(tamil_sentences[:1])\n",
        "\n",
        "print('\\nOriginal text:')\n",
        "print(french_sentences[:1])"
      ],
      "metadata": {
        "id": "YgFF7RHwFCS7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f922da2-921b-44b7-c367-51d482c659cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediciton:\n",
            "1/1 [==============================] - 1s 771ms/step\n",
            "நரி நரி தாண்டியது தாண்டியது தாண்டியது தாண்டியது தாண்டியது தாண்டியது\n",
            "\n",
            "Correct Translation:\n",
            "['அந்த கருமை நரி கண்டது ஓயாத நாயை தாண்டியது.']\n",
            "\n",
            "Original text:\n",
            "['Le renard brun saute par-dessus le chien paresseux.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional, GRU, TimeDistributed, Dense, Dropout, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "def bidirectional_embed_model(input_shape, french_vocab_size, tamil_vocab_size):\n",
        "    learning_rate = 0.005\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(french_vocab_size, 256, input_length=input_shape[1]))\n",
        "    model.add(Bidirectional(GRU(256, return_sequences=True)))\n",
        "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(tamil_vocab_size, activation='softmax')))\n",
        "\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "french_tokenized = [[1, 2, 3], [4, 5, 6, 7, 8], [9, 10, 11, 12, 13, 14]]\n",
        "tamil_padded = [[1, 2, 3, 4, 5, 6, 7, 8], [4, 5, 6, 7, 8, 9, 10, 11], [9, 10, 11, 12, 13, 14, 15, 16]]\n",
        "\n",
        "max_tamil_sequence_length = max(len(seq) for seq in tamil_padded)\n",
        "tmp_x = pad_sequences(french_tokenized, maxlen=max_tamil_sequence_length, padding='post')\n",
        "tamil_padded = pad_sequences(tamil_padded, maxlen=max_tamil_sequence_length, padding='post')\n",
        "tamil_padded = np.array(tamil_padded) - 1\n",
        "print(\"Shape of tmp_x before reshape:\", tmp_x.shape)\n",
        "print(\"Shape of tamil_padded:\", tamil_padded.shape)\n",
        "tmp_x = np.array(tmp_x)\n",
        "french_vocab_size = max(max(seq) for seq in french_tokenized) + 1\n",
        "tamil_vocab_size = max(np.max(tamil_padded), 0) + 1\n",
        "\n",
        "embed_rnn_model = bidirectional_embed_model(\n",
        "    tmp_x.shape,\n",
        "    french_vocab_size,\n",
        "    tamil_vocab_size)\n",
        "\n",
        "print(embed_rnn_model.summary())\n",
        "\n",
        "embed_rnn_model.fit(tmp_x, tamil_padded, batch_size=1024, epochs=10, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "hn16UYRqER_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f78902dc-71d0-4e9a-d841-467cce3c8d1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of tmp_x before reshape: (3, 8)\n",
            "Shape of tamil_padded: (3, 8)\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 8, 256)            3840      \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 8, 512)            789504    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " time_distributed_4 (TimeDi  (None, 8, 1024)           525312    \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 8, 1024)           0         \n",
            "                                                                 \n",
            " time_distributed_5 (TimeDi  (None, 8, 16)             16400     \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1335056 (5.09 MB)\n",
            "Trainable params: 1335056 (5.09 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - 5s 5s/step - loss: 2.7709 - accuracy: 0.1250 - val_loss: 2.8371 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 2.6196 - accuracy: 0.3125 - val_loss: 3.1137 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 2.3064 - accuracy: 0.3750 - val_loss: 4.0642 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 2.0052 - accuracy: 0.2500 - val_loss: 4.9172 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.8426 - accuracy: 0.2500 - val_loss: 5.0340 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 1.3411 - accuracy: 0.6250 - val_loss: 5.8229 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 1.1356 - accuracy: 0.6250 - val_loss: 6.7798 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.9439 - accuracy: 0.5625 - val_loss: 7.5604 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 1.0393 - accuracy: 0.6875 - val_loss: 8.1963 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.8893 - accuracy: 0.6875 - val_loss: 8.3655 - val_accuracy: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f44a3345e10>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Prediciton:\")\n",
        "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], tamil_tokenizer))\n",
        "\n",
        "print(\"\\nCorrect Translation:\")\n",
        "print(tamil_sentences[:1])\n",
        "\n",
        "print('\\nOriginal text:')\n",
        "print(french_sentences[:1])"
      ],
      "metadata": {
        "id": "j3TPYI_hE5WR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2fb9c6e-3062-4232-a5b6-844a7ca30890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediciton:\n",
            "1/1 [==============================] - 1s 710ms/step\n",
            "<PAD> அந்த கருமை நரி கண்டது ஓயாத நாயை தாண்டியது\n",
            "\n",
            "Correct Translation:\n",
            "['அந்த கருமை நரி கண்டது ஓயாத நாயை தாண்டியது.']\n",
            "\n",
            "Original text:\n",
            "['Le renard brun saute par-dessus le chien paresseux.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "embed_rnn_model.save('/content/drive/MyDrive/french_to_tamil_model')\n",
        "\n",
        "with open('/content/drive/MyDrive/ft-json/french_tokenizer.json', 'w', encoding='utf8') as f:\n",
        "    f.write(json.dumps(french_tokenizer.to_json(), ensure_ascii=False))\n",
        "\n",
        "with open('/content/drive/MyDrive/ft-json/tamil_tokenizer.json', 'w', encoding='utf8') as f:\n",
        "    f.write(json.dumps(tamil_tokenizer.to_json(), ensure_ascii=False))\n",
        "\n",
        "max_tamil_sequence_length_json = max_tamil_sequence_length\n",
        "with open('/content/drive/MyDrive/ft-json/sequence_length.json', 'w', encoding='utf8') as f:\n",
        "    f.write(json.dumps(max_tamil_sequence_length_json, ensure_ascii=False))\n"
      ],
      "metadata": {
        "id": "NNA9xwIWWNPk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}